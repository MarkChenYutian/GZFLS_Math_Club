<!DOCTYPE html>
<!-- saved from url=(0038)https://zhuanlan.zhihu.com/p/100583562 -->
<html lang="zh" data-hairline="true" data-theme="light" data-react-helmet="data-theme"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>U-Net Convolutional Network - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="keywords" content="神经网络,深度学习（Deep Learning）"><meta data-react-helmet="true" name="description" content="1. IntroductionU-net与其他机器视觉神经网络的根本区别： 一般的神经网络执行分类操作，输入一张图片，输出一个向量，其中每一项对应该图片属于对应标签的概率U-net 对图片进行“语义分割”，输出的信息不但包括…"><meta data-react-helmet="true" property="og:title" content="U-Net Convolutional Network"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/100583562"><meta data-react-helmet="true" property="og:description" content="1. IntroductionU-net与其他机器视觉神经网络的根本区别： 一般的神经网络执行分类操作，输入一张图片，输出一个向量，其中每一项对应该图片属于对应标签的概率U-net 对图片进行“语义分割”，输出的信息不但包括…"><meta data-react-helmet="true" property="og:image" content="https://pic2.zhimg.com/v2-2fae6a31e35119e1cc2dc8937fb7b133_720w.jpg?source=172ae18b"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./U-Net-Convolutional-Network-知乎_files_files/column.app.216a26f4.a35aaece18f22ce3a8ff.css" rel="stylesheet"><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><link rel="stylesheet" type="text/css" href="./U-Net-Convolutional-Network-知乎_files_files/column.user-hover-card.216a26f4.7da9107a14dfae975315.css"><link rel="stylesheet" type="text/css" href="./U-Net-Convolutional-Network-知乎_files_files/column.Labels.216a26f4.127af84d6bdb9827e439.css"><link rel="stylesheet" type="text/css" href="./U-Net-Convolutional-Network-知乎_files_files/column.modals.216a26f4.0ec427e0a4c363c48dca.css"><link rel="stylesheet" type="text/css" href="./U-Net-Convolutional-Network-知乎_files_files/column.comments-modals.216a26f4.6d6460557c91a7e3f461.css"><style data-emotion="css"></style><link rel="stylesheet" type="text/css" href="./U-Net-Convolutional-Network-知乎_files_files/column.richinput.216a26f4.ea29f2e8f601a9f98625.css"></head><body class="WhiteBg-body" data-react-helmet="class"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;chen-yu-tian-48-79&quot;}" data-zop="{&quot;authorName&quot;:&quot;Mark&quot;,&quot;itemId&quot;:100583562,&quot;title&quot;:&quot;U-Net Convolutional Network&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;100583562&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">U-Net Convolutional Network</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="Mark"><meta itemprop="image" content="https://pic1.zhimg.com/v2-fdbbabadd51b4e0417fdc94004232265_l.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/chen-yu-tian-48-79"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/chen-yu-tian-48-79"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./U-Net-Convolutional-Network-知乎_files_files/v2-fdbbabadd51b4e0417fdc94004232265_xs.jpg" srcset="https://pic1.zhimg.com/v2-fdbbabadd51b4e0417fdc94004232265_l.jpg 2x" alt="Mark"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/chen-yu-tian-48-79">Mark</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">Ad astra per aspera</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">1 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2>1. Introduction</h2><p>U-net与其他机器视觉神经网络的根本区别：</p><ul><li><b>一般的神经网络执行分类操作，输入一张图片，输出一个向量，其中每一项对应该图片属于对应标签的概率</b></li></ul><figure data-size="normal"><noscript><img src="https://picb.zhimg.com/v2-6fd1d2709c576e4a68e207dac32c3dd3_b.jpg" data-size="normal" data-rawwidth="952" data-rawheight="293" class="origin_image zh-lightbox-thumb" width="952" data-original="https://picb.zhimg.com/v2-6fd1d2709c576e4a68e207dac32c3dd3_r.jpg"/></noscript><img src="./U-Net-Convolutional-Network-知乎_files_files/v2-6fd1d2709c576e4a68e207dac32c3dd3_720w.jpg" data-size="normal" data-rawwidth="952" data-rawheight="293" class="origin_image zh-lightbox-thumb lazy" width="952" data-original="https://picb.zhimg.com/v2-6fd1d2709c576e4a68e207dac32c3dd3_r.jpg" data-actualsrc="https://picb.zhimg.com/v2-6fd1d2709c576e4a68e207dac32c3dd3_b.jpg" data-lazy-status="ok"><figcaption>图像分类示例</figcaption></figure><ul><li><b>U-net 对图片进行“语义分割”，输出的信息不但包括对图片中属性特征的识别，还包括对于指定物体位置的识别</b></li></ul><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-ef5f2558303221f7a3c0da91c7ccb3e6_b.jpg" data-size="normal" data-rawwidth="554" data-rawheight="416" class="origin_image zh-lightbox-thumb" width="554" data-original="https://pic2.zhimg.com/v2-ef5f2558303221f7a3c0da91c7ccb3e6_r.jpg"/></noscript><img src="./U-Net-Convolutional-Network-知乎_files_files/v2-ef5f2558303221f7a3c0da91c7ccb3e6_720w.jpg" data-size="normal" data-rawwidth="554" data-rawheight="416" class="origin_image zh-lightbox-thumb lazy" width="554" data-original="https://pic2.zhimg.com/v2-ef5f2558303221f7a3c0da91c7ccb3e6_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ef5f2558303221f7a3c0da91c7ccb3e6_b.jpg" data-lazy-status="ok"><figcaption>语义分割示例</figcaption></figure><p>过去进行对图像进行语义分割的一种尝试是：通过提供一个像素周边的环境，训练网络识别这个像素属于的物体的类型。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-b0aafce36a77cbf3356d7a0d81c6136e_b.jpg" data-caption="" data-size="normal" data-rawwidth="1116" data-rawheight="351" class="origin_image zh-lightbox-thumb" width="1116" data-original="https://pic1.zhimg.com/v2-b0aafce36a77cbf3356d7a0d81c6136e_r.jpg"/></noscript><img src="./U-Net-Convolutional-Network-知乎_files_files/v2-b0aafce36a77cbf3356d7a0d81c6136e_720w.jpg" data-caption="" data-size="normal" data-rawwidth="1116" data-rawheight="351" class="origin_image zh-lightbox-thumb lazy" width="1116" data-original="https://pic1.zhimg.com/v2-b0aafce36a77cbf3356d7a0d81c6136e_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-b0aafce36a77cbf3356d7a0d81c6136e_b.jpg" data-lazy-status="ok"></figure><p>这种方法确实可行，因为可以对图片中的每一个像素进行同样的操作来逐个识别每个像素属于什么物体。但是这个方法有两个不足之处:</p><ul><li><b>处理速度太慢 </b>因为神经网络每次要单独训练，而且划分的Patch之间有很大的冗余重复</li></ul><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-e17a458cda43e4121e45c595c3b53926_b.jpg" data-caption="" data-size="normal" data-rawwidth="777" data-rawheight="608" class="origin_image zh-lightbox-thumb" width="777" data-original="https://pic1.zhimg.com/v2-e17a458cda43e4121e45c595c3b53926_r.jpg"/></noscript><img src="./U-Net-Convolutional-Network-知乎_files_files/v2-e17a458cda43e4121e45c595c3b53926_720w.jpg" data-caption="" data-size="normal" data-rawwidth="777" data-rawheight="608" class="origin_image zh-lightbox-thumb lazy" width="777" data-original="https://pic1.zhimg.com/v2-e17a458cda43e4121e45c595c3b53926_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e17a458cda43e4121e45c595c3b53926_b.jpg" data-lazy-status="ok"></figure><ul><li><b>网络的设计需要在确定位置和更大的感受野之间权衡</b> 当使用尺寸更大的Patch时，需要使用更多的池化层，会降低网络确定像素位置的能力；当网络使用尺寸较小的Patch时，神经网络的感受野（对周边特征的“视野”）会变小。</li></ul><p>与之前方法相比，U-net有以下几点特征：</p><ul><li>我们的网络在上采样部分依然有大量的特征通道，这使得网络可以将空间上下文信息向更高的分辨率层传播。结果是，上采样路径基本对称于下采样路径，并呈现出一个U型。</li><li>网络不存在任何全连接层，并且，只使用每个卷积的valid部分，例如，分割图只包含这样一些像素点，这些像素点的完整上下文都出现在输入图像中。这种策略允许使用Overlap-tile策略无缝地分割任意大小的图像(参见下图)。</li><li>为了预测图像边界区域的像素点，我们采用镜像图像的方式补全缺失的环境像素。这个tiling方法在使用网络分割大图像时是非常有用的，因为如果不这么做，GPU显存会限制图像分辨率。</li><li>此外，为了使得U-net可以在小批量数据的情况下顺利训练，我们使用了弹性形变处理训练图像（因为生物材料经常发生形变），通过这种方法，我们可以极大地增加训练图像的数量。</li><li>为了应对细胞分割任务中的一个挑战是如何将同类别的相互接触的目标分开，为了解决这个问题，我们改进了网络的损失函数，提出了使用一种带权重的损失(weighted loss)；在新的损失函数中，分割相互接触的细胞会获得更大的权重。</li></ul><h2>2 Network Architecture</h2><p>因此，本文作者基于“完全卷积网络”设计了 U-net 神经网络架构（如下图），用以处理图像的语义分割问题</p><figure data-size="normal"><noscript><img src="https://picb.zhimg.com/v2-4fb895141e2322cf7ba21292e0984368_b.jpg" data-size="normal" data-rawwidth="1269" data-rawheight="859" class="origin_image zh-lightbox-thumb" width="1269" data-original="https://picb.zhimg.com/v2-4fb895141e2322cf7ba21292e0984368_r.jpg"/></noscript><img src="./U-Net-Convolutional-Network-知乎_files_files/v2-4fb895141e2322cf7ba21292e0984368_720w.jpg" data-size="normal" data-rawwidth="1269" data-rawheight="859" class="origin_image zh-lightbox-thumb lazy" width="1269" data-original="https://picb.zhimg.com/v2-4fb895141e2322cf7ba21292e0984368_r.jpg" data-actualsrc="https://picb.zhimg.com/v2-4fb895141e2322cf7ba21292e0984368_b.jpg" data-lazy-status="ok"><figcaption>U-net 网络架构，图片引用自“U-Net: Convolutional Networks for Biomedical Image Segmentation, Fig 1”</figcaption></figure><p>在这个网络中，图片首先通过最大池化层进行“下采样”，减小图片分辨率的同时使用3*3卷积核提取图片的特征，每一层中，提取的特征通道数量是输入特征通道数量的两倍。通过这种方法，最后将输入图片转化为大小28*28，具有1024个特征通道状态。</p><p>接着，网络通过2*2的卷积核对图片进行上采样（如下图所示，但下图中使用的是3*3卷积核）</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-a8b996405d118cfa283b6f95a866ffd7_b.gif" data-size="normal" data-rawwidth="355" data-rawheight="403" data-thumbnail="https://pic2.zhimg.com/v2-a8b996405d118cfa283b6f95a866ffd7_b.jpg" class="content_image" width="355"/></noscript><div class="RichText-gifPlaceholder"><div class="GifPlayer" data-size="normal" data-za-detail-view-path-module="GifItem"><img class="ztext-gif" role="presentation" src="./U-Net-Convolutional-Network-知乎_files_files/v2-a8b996405d118cfa283b6f95a866ffd7_b.jpg" data-thumbnail="https://pic2.zhimg.com/v2-a8b996405d118cfa283b6f95a866ffd7_b.jpg" data-size="normal"><svg width="60" height="60" viewBox="0 0 60 60" class="GifPlayer-icon"><g fill="none" fill-rule="evenodd"><ellipse fill="#000" opacity="0.45" cx="30" cy="30" rx="30" ry="30"></ellipse><ellipse stroke="#FFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" stroke-dasharray="4,1,4" cx="30" cy="30" rx="26" ry="26"></ellipse><svg x="16" y="18.5" class="GifPlayer-icon"><path d="M12.842 12.981V11.4H7.64v1.653h3.27v.272c-.018 1.881-1.442 3.147-3.516 3.147-2.382 0-3.876-1.846-3.876-4.834 0-2.936 1.485-4.79 3.832-4.79 1.732 0 2.936.835 3.428 2.364h1.977c-.43-2.566-2.522-4.201-5.405-4.201-3.55 0-5.845 2.601-5.845 6.644 0 4.096 2.268 6.654 5.863 6.654 3.322 0 5.475-2.083 5.475-5.327zM17.518 18V5.317H15.55V18h1.97zm5.142 0v-5.256h5.449v-1.74h-5.45V7.11h5.95V5.317h-7.918V18h1.969z" fill="#fff"></path></svg></g></svg></div></div><figcaption>使用3*3卷积核进行上采样（将2*2图像上采样为5*5图像）</figcaption></figure><p>需要注意的是在上采样的过程中，图片依然保存了大量的特征通道，这有助于网络将图片周边的环境向上传播</p><p>与此同时，为了提高图片的分辨率，网络还会将池化前的特征通道<b>经过裁剪后（裁剪是由于无填充像素下卷积操作带来的无法避免的尺寸减小导致的，如下图所示）</b>复制到上采样时的状态中，再使用一个3*3卷积核将这些特征通道进一步整合起来。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-4be57777206c41921a9262ec3f484678_b.gif" data-size="normal" data-rawwidth="265" data-rawheight="259" data-thumbnail="https://pic4.zhimg.com/v2-4be57777206c41921a9262ec3f484678_b.jpg" class="content_image" width="265"/></noscript><div class="RichText-gifPlaceholder"><div class="GifPlayer" data-size="normal" data-za-detail-view-path-module="GifItem"><img class="ztext-gif" role="presentation" src="./U-Net-Convolutional-Network-知乎_files_files/v2-4be57777206c41921a9262ec3f484678_b.jpg" data-thumbnail="https://pic4.zhimg.com/v2-4be57777206c41921a9262ec3f484678_b.jpg" data-size="normal"><svg width="60" height="60" viewBox="0 0 60 60" class="GifPlayer-icon"><g fill="none" fill-rule="evenodd"><ellipse fill="#000" opacity="0.45" cx="30" cy="30" rx="30" ry="30"></ellipse><ellipse stroke="#FFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" stroke-dasharray="4,1,4" cx="30" cy="30" rx="26" ry="26"></ellipse><svg x="16" y="18.5" class="GifPlayer-icon"><path d="M12.842 12.981V11.4H7.64v1.653h3.27v.272c-.018 1.881-1.442 3.147-3.516 3.147-2.382 0-3.876-1.846-3.876-4.834 0-2.936 1.485-4.79 3.832-4.79 1.732 0 2.936.835 3.428 2.364h1.977c-.43-2.566-2.522-4.201-5.405-4.201-3.55 0-5.845 2.601-5.845 6.644 0 4.096 2.268 6.654 5.863 6.654 3.322 0 5.475-2.083 5.475-5.327zM17.518 18V5.317H15.55V18h1.97zm5.142 0v-5.256h5.449v-1.74h-5.45V7.11h5.95V5.317h-7.918V18h1.969z" fill="#fff"></path></svg></g></svg></div></div><figcaption>无填充像素的卷积必然会带来边缘处尺寸的收缩（5*5图像经过3*3卷积核步长为1的卷积后尺寸收缩为2*2）</figcaption></figure><p>如此重复此步骤直到图片的尺寸重新被放大至392*392 pixel （作为对比，输入是572*572像素，大小的减少是由于重复卷积操作导致的），此时图片依然有多达64个特征通道。</p><p>此时，我们可以看到上采样的通道与下采样的通道基本对称，整个神经网络呈现为“U形”</p><p>这时候再用3*3卷积核与1*1卷积核将这些特征通道重新整合起来即可。</p><p class="ztext-empty-paragraph"><br></p><blockquote>需要注意的一点是，再U-net中，作者使用的所有激活函数都是线性修正单元（ReLU, rectified linear unit）。由于使用这种函数的一阶导数是常数，与带权输入无关，可以有效避免深度网络训练时的梯度消失问题与梯度爆炸问题（与之相对应，Sigmoid函数有严重的梯度消失问题，这会极大的降低神经元的学习速度，同时使得神经网络靠近输出的层级更加容易达到饱和状态）。</blockquote><h2>3 Training</h2><p>U-net使用随机梯度下降的方法进行训练，每批次中只有一幅图片。为了减少使用单个图片计算损失函数梯度带来的，每次梯度下降方向由于图片之间差异带来的过大变化，作者使用了很大的动量值(momentum = 0.99) ，这意味着每一次梯度下降的方向更多的根据过去的图片的损失函数梯度来决定。</p><p>同时，为了使得神经网络可以区分开相邻的两个细胞，作者改进了交叉熵函数。作者通过在交叉熵函数中增加了单个像素的权重值使得网络更加注重细胞之间的分割（这里看不懂具体实现方法）</p><p>U-net使用了输入图片每一个像素的经过改进的交叉熵函数的柔性最大值作为整幅图片的损失函数。</p></div></div><div class="ContentItem-time">发布于 01-01</div><div class="Post-topicsAndReviewer"></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 520px;"></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 0px;"></div></div></article></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex CornerAnimayedFlex--hidden"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" aria-label="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><div><div><div class="css-8pdeid"></div></div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete12-0" id="Popover11-toggle" aria-haspopup="true" aria-owns="Popover11-content" class="Input" placeholder="选择语言" value=""><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></label></div></div></div></div></div></body></html>